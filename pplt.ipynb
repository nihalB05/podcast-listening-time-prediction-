{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihalB05/podcast-listening-time-prediction-/blob/main/pplt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install category_encoders"
      ],
      "metadata": {
        "id": "dekQnEAZmNHf"
      },
      "id": "dekQnEAZmNHf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Imports and Setup\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# 2. Load Data\n",
        "df_train = pd.read_csv(\"/content/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# 3. Initial Exploration (optional quick checks)\n",
        "print(df_train.info()); print(df_train.isnull().sum())\n",
        "print(df_test.info()); print(df_test.isnull().sum())\n",
        "\n",
        "# 4. Data Cleaning & Feature Engineering\n",
        "# 4.1 Drop tiny missing rows or impute\n",
        "# Number_of_Ads has one null in train: drop\n",
        "df_train.dropna(subset=[\"Number_of_Ads\"], inplace=True)\n",
        "\n",
        "# 4.2 Round Number_of_Ads to integer\n",
        "for df in (df_train, df_test):\n",
        "    df.loc[:, 'Number_of_Ads'] = df['Number_of_Ads'].round().astype(int)\n",
        "\n",
        "# 4.3 Remove extreme outliers in Number_of_Ads (>50)\n",
        "    df_train.drop(df_train[df_train['Number_of_Ads'] >= 50].index, inplace=True)\n",
        "\n",
        "# 4.4 Fill missing Episode_Length_minutes by Genre median\n",
        "for df in (df_train, df_test):\n",
        "    df['Episode_Length_minutes'] = df.groupby('Genre')['Episode_Length_minutes'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# 4.5 Guest_Popularity: fill missing with -1 + missing flag\n",
        "for df in (df_train, df_test):\n",
        "    df['Guest_Popularity_percentage'].fillna(-1, inplace=True)\n",
        "    df['Guest_Popularity_missing'] = (df['Guest_Popularity_percentage'] == -1).astype(int)\n",
        "\n",
        "# 4.6 Extract Episode_Number from Episode_Title\n",
        "extract_episode = lambda t: int(re.search(r\"Episode\\s*(\\d+)\", str(t), re.IGNORECASE).group(1)) if re.search(r\"Episode\\s*(\\d+)\", str(t), re.IGNORECASE) else 0\n",
        "for df in (df_train, df_test):\n",
        "    df['Episode_Number'] = df['Episode_Title'].apply(extract_episode)\n",
        "\n",
        "# 4.7 Interaction features\n",
        "df_train['host_guest_popularity'] = df_train['Host_Popularity_percentage'] * df_train['Guest_Popularity_percentage']\n",
        "df_train['length_ads_ratio'] = df_train['Episode_Length_minutes'] / (df_train['Number_of_Ads'] + 1)\n",
        "# same on test\n",
        "for df in (df_test,):\n",
        "    df['host_guest_popularity'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n",
        "    df['length_ads_ratio'] = df['Episode_Length_minutes'] / (df['Number_of_Ads'] + 1)\n",
        "genre_counts=df_train[\"Genre\"].value_counts().reset_index()\n",
        "\n",
        "genre_counts.columns = ['Genre', 'Count']\n",
        "\n",
        "fig= go.Figure(data=[go.Pie(\n",
        "    labels=genre_counts['Genre'],\n",
        "    values=genre_counts['Count'],\n",
        "    hole=0.4,  # Adjust the 'hole' value to make it a donut chart (0 for pie)\n",
        "    title='Podcast Count by Genre',\n",
        "    hoverinfo='label+percent',\n",
        "    textinfo='percent',\n",
        "    marker=dict(colors=px.colors.sequential.Plasma), #Use a color scale.\n",
        "\n",
        ")])\n",
        "\n",
        "fig.show()\n",
        "# Define your specific bin edges\n",
        "bin_edges = [0, 1, 2, 3, 4, 5, 13, float('inf')] #float('inf') to capture all values greater than 13.\n",
        "\n",
        "# Define corresponding bin labels\n",
        "bin_labels = ['1', '2', '3', '4', '5', '6-13', '14+']\n",
        "\n",
        "# Create the bins\n",
        "df_train['Ads_Bins'] = pd.cut(df_train['Number_of_Ads'], bins=bin_edges, labels=bin_labels, right=False) #right=False, so that the bin includes the lower value.\n",
        "\n",
        "# Create the count plot\n",
        "sns.countplot(x='Ads_Bins', data=df_train)\n",
        "plt.show()\n",
        "\n",
        "plt.boxplot(df_train[\"Number_of_Ads\"])\n",
        "plt.xlabel(\"distribution of Number of Ads \")\n",
        "plt.ylabel(\"Number of ads \")\n",
        "\n",
        "\n",
        "# 4.8 Binning Number_of_Ads (optional; keep continuous too)\n",
        "# Already using continuous; bins commented out for now\n",
        "# 4.9 Target encode Podcast_Name\n",
        "te = TargetEncoder(cols=['Podcast_Name'])\n",
        "df_train['Podcast_Name_encoded'] = te.fit_transform(df_train['Podcast_Name'], df_train['Listening_Time_minutes'])\n",
        "df_test['Podcast_Name_encoded'] = te.transform(df_test['Podcast_Name'])\n",
        "\n",
        "# 4.10 Drop unused columns\n",
        "drop_cols = ['id', 'Episode_Title', 'Podcast_Name']\n",
        "df_train.drop(columns=[c for c in drop_cols if c in df_train.columns], inplace=True)\n",
        "df_test_ids = df_test['id'].copy()\n",
        "df_test.drop(columns=[c for c in drop_cols if c in df_test.columns], inplace=True)\n",
        "\n",
        "# 5. One-Hot Encode Categorical Features manually\n",
        "cats = ['Genre','Publication_Day','Episode_Sentiment', 'Publication_Time']\n",
        "for col in cats:\n",
        "    df_train = pd.get_dummies(df_train, columns=[col], prefix=col)\n",
        "    df_test = pd.get_dummies(df_test, columns=[col], prefix=col)\n",
        "\n",
        "# Ensure train/test have same columns\n",
        "test_extra = set(df_train.columns) - set(df_test.columns)\n",
        "for c in test_extra:\n",
        "    df_test[c] = 0\n",
        "\n",
        "# 6. Prepare Features & Target\n",
        "y = df_train['Listening_Time_minutes']\n",
        "X = df_train.drop(columns=['Listening_Time_minutes'])\n",
        "\n",
        "# 7. Train/Validation Split\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "\n",
        "# 8. Preprocessing Pipeline for numeric scaling only\n",
        "numeric_features = x_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "# exclude booleans already encoded\n",
        "dummy_cols = x_train.select_dtypes(include=['uint8','bool']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features)\n",
        "], remainder='passthrough')  # pass through dummy columns\n",
        "\n",
        "# 9. Preprocess Train/Val Sets\n",
        "x_train_pre = preprocessor.fit_transform(x_train)\n",
        "x_val_pre   = preprocessor.transform(x_val)\n",
        "\n",
        "df_test_pre = preprocessor.transform(df_test)\n",
        "\n",
        "# 10. Train Best XGBoost Model\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='reg:squarederror',\n",
        "    eval_metric='rmse',\n",
        "    random_state=69,\n",
        "    early_stopping_rounds=30\n",
        ")\n",
        "xgb_model.fit(\n",
        "    x_train_pre, y_train,\n",
        "    eval_set=[(x_val_pre, y_val)],\n",
        "    verbose=20\n",
        ")\n",
        "\n",
        "# 11. Validation Evaluation\n",
        "y_pred_val = xgb_model.predict(x_val_pre)\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "rmse_val = np.sqrt(mse_val)\n",
        "print(f\"Validation RMSE: {rmse_val:.4f}\")\n",
        "\n",
        "# 12. Final Model Training on Full Train Data\n",
        "# Combine full train\n",
        "full_pre = preprocessor.fit_transform(X)\n",
        "\n",
        "xgb_final = XGBRegressor(\n",
        "    n_estimators=xgb_model.best_iteration,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=69\n",
        ")\n",
        "xgb_final.fit(full_pre, y)\n",
        "\n",
        "# 13. Predict on Test and Create Submission\n",
        "final_preds = xgb_final.predict(df_test_pre)\n",
        "submission = pd.DataFrame({'id': df_test_ids, 'Listening_Time_minutes': final_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission.csv saved.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9_jjh8kGcK8",
        "outputId": "8fd42767-6ac4-4cea-a150-864e40714213"
      },
      "id": "V9_jjh8kGcK8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 750000 entries, 0 to 749999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   id                           750000 non-null  int64  \n",
            " 1   Podcast_Name                 750000 non-null  object \n",
            " 2   Episode_Title                750000 non-null  object \n",
            " 3   Episode_Length_minutes       662907 non-null  float64\n",
            " 4   Genre                        750000 non-null  object \n",
            " 5   Host_Popularity_percentage   750000 non-null  float64\n",
            " 6   Publication_Day              750000 non-null  object \n",
            " 7   Publication_Time             750000 non-null  object \n",
            " 8   Guest_Popularity_percentage  603970 non-null  float64\n",
            " 9   Number_of_Ads                749999 non-null  float64\n",
            " 10  Episode_Sentiment            750000 non-null  object \n",
            " 11  Listening_Time_minutes       750000 non-null  float64\n",
            "dtypes: float64(5), int64(1), object(6)\n",
            "memory usage: 68.7+ MB\n",
            "None\n",
            "id                                  0\n",
            "Podcast_Name                        0\n",
            "Episode_Title                       0\n",
            "Episode_Length_minutes          87093\n",
            "Genre                               0\n",
            "Host_Popularity_percentage          0\n",
            "Publication_Day                     0\n",
            "Publication_Time                    0\n",
            "Guest_Popularity_percentage    146030\n",
            "Number_of_Ads                       1\n",
            "Episode_Sentiment                   0\n",
            "Listening_Time_minutes              0\n",
            "dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250000 entries, 0 to 249999\n",
            "Data columns (total 11 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   id                           250000 non-null  int64  \n",
            " 1   Podcast_Name                 250000 non-null  object \n",
            " 2   Episode_Title                250000 non-null  object \n",
            " 3   Episode_Length_minutes       221264 non-null  float64\n",
            " 4   Genre                        250000 non-null  object \n",
            " 5   Host_Popularity_percentage   250000 non-null  float64\n",
            " 6   Publication_Day              250000 non-null  object \n",
            " 7   Publication_Time             250000 non-null  object \n",
            " 8   Guest_Popularity_percentage  201168 non-null  float64\n",
            " 9   Number_of_Ads                250000 non-null  float64\n",
            " 10  Episode_Sentiment            250000 non-null  object \n",
            "dtypes: float64(4), int64(1), object(6)\n",
            "memory usage: 21.0+ MB\n",
            "None\n",
            "id                                 0\n",
            "Podcast_Name                       0\n",
            "Episode_Title                      0\n",
            "Episode_Length_minutes         28736\n",
            "Genre                              0\n",
            "Host_Popularity_percentage         0\n",
            "Publication_Day                    0\n",
            "Publication_Time                   0\n",
            "Guest_Popularity_percentage    48832\n",
            "Number_of_Ads                      0\n",
            "Episode_Sentiment                  0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e24333e64beb>:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Guest_Popularity_percentage'].fillna(-1, inplace=True)\n",
            "<ipython-input-11-e24333e64beb>:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Guest_Popularity_percentage'].fillna(-1, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-rmse:25.09733\n",
            "[20]\tvalidation_0-rmse:13.46522\n",
            "[40]\tvalidation_0-rmse:13.13505\n",
            "[60]\tvalidation_0-rmse:13.10053\n",
            "[80]\tvalidation_0-rmse:13.08252\n",
            "[100]\tvalidation_0-rmse:13.07095\n",
            "[120]\tvalidation_0-rmse:13.06099\n",
            "[140]\tvalidation_0-rmse:13.05559\n",
            "[160]\tvalidation_0-rmse:13.04650\n",
            "[180]\tvalidation_0-rmse:13.03921\n",
            "[200]\tvalidation_0-rmse:13.03331\n",
            "[220]\tvalidation_0-rmse:13.02539\n",
            "[240]\tvalidation_0-rmse:13.02198\n",
            "[260]\tvalidation_0-rmse:13.01546\n",
            "[280]\tvalidation_0-rmse:13.01115\n",
            "[300]\tvalidation_0-rmse:13.00634\n",
            "[320]\tvalidation_0-rmse:13.00257\n",
            "[340]\tvalidation_0-rmse:12.99901\n",
            "[360]\tvalidation_0-rmse:12.99440\n",
            "[380]\tvalidation_0-rmse:12.99325\n",
            "[400]\tvalidation_0-rmse:12.99018\n",
            "[420]\tvalidation_0-rmse:12.98740\n",
            "[440]\tvalidation_0-rmse:12.98340\n",
            "[460]\tvalidation_0-rmse:12.98111\n",
            "[480]\tvalidation_0-rmse:12.97983\n",
            "[499]\tvalidation_0-rmse:12.97623\n",
            "Validation RMSE: 12.9762\n",
            "Submission.csv saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceY_PSqnYDJY",
        "outputId": "eed341fa-08dc-4afe-8d7b-9ab38f664788"
      },
      "id": "ceY_PSqnYDJY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}